目标

+ 理解基于模型的强化学习方法。

+ 掌握策略评估、策略改进、策略迭代和值迭代四大核心算法。

+ 能手推 Bellman 方程，并理解其收敛性。

## 1. 策略评估

目标：给定一个策略 $\pi$，计算其对应的状态价值函数 $V^\pi(s)$

+ 使用 Bellman 期望方程：

$$V^\pi(s)=\sum_{a}{\pi(a|s)\sum_{s',r}{p(s',r|s,a)[r+\gamma V^\pi(s')]}}$$

+ 迭代更新公式（同步备份）

$$V_{k+1}(s) <- \sum_{a}{\pi(a|s)\sum_{s',r}{p(s',r|s,a)[r + \gamma V_k(s')]}}$$

+ 算法流程：

1. 初始化 $V_0(s)=0$（对所有 $s \in S$）

2. 重复以下步骤直到收敛：

+ 对每个状态 $s$：
  + 计算新价值 $V_{k+1}(s)$如上式

3. 输出 $V^\pi(s) \approx V_k(s)$

## 2. 策略改进

目标：给定当前价值函数 $V^\pi$，构造一个更好或相等的新策略 $\pi'$

+ 贪心策略改进：

$$\pi's(s)=arg max_{a}{\sum_{s',r}{p(s',r|s,a)[r+\gamma V^\pi(s')]}}$$

+ 策略改进定理：

  + 如果对所有 $s$，有

$$\sum_{s',r}{p(s',r|s,\pi'(s))[r+\gamma V^\pi(s')]} >= V^\pi(s)$$

+ 若严格大于某个 $s$，则新策略严格更好

## 3. 策略迭代

目标：交替进行策略评估 + 策略改进，直到策略不再变化。

+ 算法流程：

1. 初始化任意策略 $\pi_0$

2. 循环：

+ 策略评估：计算 $V^{\pi k}$

+ 策略改进：得到新策略

$$\pi_{k+1}(s)=arg max_{a}{\sum_{s',r}{p(s',r|s,a)[r + \gamma V^{\pi k}(s')]}}$$

3. 输出最优策略 $\pi^*$

+ 性质：

  + 每次迭代策略严格改进（除非已最优）

  + 有限步内收敛（因为策略数量有限）

4. 值迭代

目标：跳过显式策略表示，直接迭代逼近最优价值函数 $V^*$

+ 基于 Bellman 最优方程：

$$V^*(s)=max_{a}{\sum_{s',r}{p(s',r|s,a)[r+\gamma V^*(s')]}}$$

+ 迭代更新公式：

$$V_{k+1}(s) <- max_{a}{\sum_{s',r}{p(s',r|s,a)[r+\gamma V_k(s')]}}$$

+ 算法流程

1. 初始化 $V_0(s)=0$

2. 重复直到收敛：

+ 对每个状态 $s$:

  + 执行上述 max 更新

3. 得到 $V^* \approx V_k$

4. 提取最优策略

$$\pi^*(s)=arg max_{a}{\sum_{s',r}{p(s',r|s,a)[r+\gamma V^*(s')]}}$$

## 6. DP 的局限性

+ ❌ 需要完整环境模型：必须知道 $p(s',r|s,a)$

+ ❌ 维度灾难（Curse of Dimensionality）：

  + 状态空间大时（如图像、连续状态），无法存储/遍历所有 $V(s)$

+ ✅ 但它是理论基石：后续无模型方法（如 Q-Learning）都试图“模仿”DP 的效果
