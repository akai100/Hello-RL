## 强化学习的定义

强化学习是一种通过试错（trial and error）来学习如何做出一系列决策的方法

主要组件：

+ Agent(智能体)：执行动作的实体

+ Environment（环境）：智能体所处的时间或情境

+ Action（动作）：智能体可以采取的行为

+ State（状态）：环境当前的情况

+ Reward（奖励）：环境对智能体行为的反馈，用于评估行为的好坏


## 马尔科夫决策过程（MDP）

+ MDP是描述强化学习问题的数学框架

+ MDP 五元组： $(S, A, P, R,  \gamma)$

  + $S$：状态集合
 
  + $A$：动作集合
 
  + $P(s',r|s,a)$：从状态 $s$ 采取动作 $a$ 后转移到状态 $s'$，并获得奖励 $r$的概率。
 
  + $R(s,a)$：从状态 $s$采取动作 $a$后的期望奖励
 
  + $\gamma$：折扣因子，通常 $0 <= \gamma < 1$，用于计算未来奖励的现值

## 回报（Return） vs 折扣回报（Discounted Return）

+ 回报：一个策略下从某一时刻开始所有未来奖励的总和

$$G_t=\sum_{k=0}{r_{t+k+1}}$$

+ 折扣回报：考虑到未来奖励的不确定性，引入折扣因子 $\gamma$ 来减少未来奖励的重要性

$$G_t = \sum_{k=0}{\gamma^kr_{t+k+1}}$$

## 策略（Policy）

+ 策略 $\pi(a|s)$定义了智能体在给定状态下选择每个可能动作的概率分布

+ 目标是找到最优策略 $\pi^*$，使得累积回报最大

## 状态价值函数 V(s) 和动作价值函数 Q(s,a)

+ 状态价值函数 $V^\pi(s)$表示从状态 $s$开始，按照策略 \pi 进行行动的期望回报

  + $V\pi(s)=E_\pi[G_t|S_t=s]$

+ 动作价值函数 $Q^\pi(s,a)$表示从状态 $s$开始，采取动作 $a$，然后按照策略 $\pi$进行行动的期望回报

   + $Q^\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]$

## 最大化期望累积回报

+ 智能体的目标是找到一种策略，使累积回报的期望值最大化

+ 这通常涉及到平衡短期奖励和长期收益之间的权衡
