## K 臂赌博机问题定义

+ 假设你面前有 K 台老虎机（Bandits），每台机器有一个未知的概率分布决定每次拉动时的奖励

+  目标是通过有限次数的尝试，找到奖励最高的那台机器，并最大化总奖励

## 累积悔值（Regret）概念

+ 悔值：在某一步中，选择了一个非最优动作而错过的最大可能奖励

+ 累积悔值：所有步数中的悔值之和。理想情况下，我们希望累积悔值尽可能小

## ε-greedy 策略

+ 思想：大部分时间选择当前估计的最佳动作（exploit），但以一定概率 ϵ 随机选择其他动作（explore）

+ 算法步骤：

1. 初始化每个动作的价值估计 $Q(a)=0$，计数器 $N(a)=0$

2. 对于每一步 t：

   + 生成一个随机数 $p \in [0, 1]$
  
   + 如果 $p < \epsilon$, 随机选择一个动作 $a$
  
   + 否则，选择当前估计的最佳动作 $a_t=arg max_a Q(a)$
  
   + 执行动作 $a_t$，观察奖励 $r_t$
  
   + 更新价值估计： $Q(a_t) <- Q(a_t)+\frac{1}{N(a_t)}(r_t-Q(a_t))$
  
   + 更新计数器： $N(a_t) <- N(a_t)+1$

  ## Softmax 探索

  + 思想：根据每个动作的当前价值估计来分配选择概率，高价值的动作被选中的概率更高，但仍有一定概率选择低价值的动作。

  + 公式：

$$P(a)=\frac{exp(Q(a)/\tau}{\sum_{a'}{exp(Q(a')/\tau)}}$$

其中：

+ $\tau$ 是温度参数，控制搜索的程度，当 $\tau \to 0$，接近贪心策略， 当 $\tau \to \infty$，接近均匀随机选择。

## UCB（Upper Confidence Bound）算法

+ 思想：不仅考虑动作的历史平均奖励，还增加一项表示对该动作了解程度的置信区间宽度。对于不确定性大的动作给予更高的优先级。

+ 公式：

$$A_t=arg max_{a}[Q(a)+c\sqrt{\frac{log t}{N(a)}}]$$

其中：

+ $  c  $   是控制探索强度的常数

## Thompson Sampling（贝叶斯视角）

+ 思想：为每个动作假设一个先验分布，基于历史数据更新后验分布，然后从后验分布中采样，选择样本最大的动作。

+ 步骤：

6. 初始化每个动作的先验分布（例如 Beta 分布）

7. 对于每一步 t：

   + 从每个动作的后验分布中采样得到 $\theta_a$
  
   + 选择采样值最大的动作 $a_t=arg max_a \theta_a$
  
   + 执行动作 $a_t$，观察奖励 $r_t$
  
   + 根据新的奖励更新该动作的后验分布
